{
 "cells": [
  {
   "cell_type": "raw",
   "id": "74198406",
   "metadata": {},
   "source": [
    "---\n",
    "title: Assignment 04\n",
    "author:\n",
    "  - name: Gavin Boss\n",
    "    affiliations:\n",
    "      - id: bu\n",
    "        name: Boston University\n",
    "        city: Boston\n",
    "        state: MA\n",
    "number-sections: true\n",
    "date: '2025-10-04'\n",
    "date-modified: today\n",
    "date-format: long\n",
    "format:\n",
    "  docx:\n",
    "    toc: true\n",
    "    toc-depth: 2\n",
    "  html:\n",
    "    theme: cerulean\n",
    "    toc: true\n",
    "    toc-depth: 2\n",
    "execute:\n",
    "  echo: false\n",
    "  eval: true\n",
    "  freeze: auto\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b950b77",
   "metadata": {},
   "source": [
    "# Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "pio.renderers.default = \"notebook+notebook_connected+vscode\"\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n",
    "\n",
    "# Load Data\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"data/lightcast_job_postings.csv\")\n",
    "\n",
    "# Show Schema and Sample Data\n",
    "# print(\"---This is Diagnostic check, No need to print it in the final doc---\")\n",
    "\n",
    "# df.printSchema() # comment this line when rendering the submission\n",
    "# df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23045865",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: true\n",
    "#| echo: false\n",
    "#| fig-align: center\n",
    "\n",
    "from pyspark.sql.functions import col, pow\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Drop Missing Values in target variable and key features\n",
    "\n",
    "df = df.dropna(subset=[\n",
    "    \"SALARY\", \"MIN_YEARS_EXPERIENCE\",\"EMPLOYMENT_TYPE_NAME\",\"REMOTE_TYPE_NAME\",\n",
    "    \"DURATION\", \"IS_INTERNSHIP\",\"COMPANY_IS_STAFFING\", \"STATE_NAME\", \"MIN_EDULEVELS_NAME\"\n",
    "])\n",
    "\n",
    "# Selecting 2 categorical variables\n",
    "categorical_cols =[\n",
    "    \"REMOTE_TYPE_NAME\", \"EMPLOYMENT_TYPE_NAME\"\n",
    "]\n",
    "\n",
    "#Index and One-Hot encode\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_idx\", handleInvalid='skip') for col in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{col}_idx\", outputCol=f\"{col}_vec\") for col in categorical_cols]\n",
    "\n",
    "# Assemble base features (for GLR and Random Forrest)\n",
    "from pyspark.sql.functions import col, pow\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"MIN_YEARS_EXPERIENCE\", \"DURATION\",\n",
    "        \"IS_INTERNSHIP\", \"COMPANY_IS_STAFFING\"\n",
    "    ] + [f\"{col}_vec\" for col in categorical_cols],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Build pipeline and transform\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "data = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Create squared term for Polynomical Regression\n",
    "data = data.withColumn(\"MIN_YEARS_EXPERIENCE_SQ\", pow(col(\"MIN_YEARS_EXPERIENCE\"),2))\n",
    "\n",
    "#Assemble polynomial features\n",
    "assembler_poly = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"MIN_YEARS_EXPERIENCE\", \"DURATION\",\n",
    "        \"IS_INTERNSHIP\", \"COMPANY_IS_STAFFING\"\n",
    "    ] + [f\"{col}_vec\" for col in categorical_cols],\n",
    "    outputCol=\"features_poly\"\n",
    ")\n",
    "\n",
    "data = assembler_poly.transform(data)\n",
    "\n",
    "data.select(\"SALARY\",\"features\", \"features_poly\").show(5, truncate=False)\n",
    "\n",
    "# Split into training and testing sets (80% training, 20% testing)\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Check number of records in each\n",
    "print(\"Training Data Count:\", train_data.count())\n",
    "print(\"Testing Data Count:\", test_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deda921f",
   "metadata": {},
   "source": [
    "# Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63efeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Check number of records in each\n",
    "print(\"Training Data Count:\", train_data.count())\n",
    "print(\"Testing Data Count:\", test_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8765c",
   "metadata": {},
   "source": [
    "- I chose the 80/20 split that was discussed during class, the data set is large enough that this split should be fine, and should account for enough of the data.\n",
    "\n",
    "# Regression_DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3aaf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, trim, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "regression_df = df.select(\n",
    "    \"SALARY\",\n",
    "    \"MIN_YEARS_EXPERIENCE\",\n",
    "    \"MAX_YEARS_EXPERIENCE\",\n",
    "    \"DURATION\",\n",
    "    \"IS_INTERNSHIP\",\n",
    "    \"COMPANY_IS_STAFFING\",\n",
    "    \"REMOTE_TYPE_NAME\",\n",
    "    \"EMPLOYMENT_TYPE_NAME\",\n",
    "    \"EDUCATION_LEVELS_NAME\"\n",
    ")\n",
    "\n",
    "regression_df = regression_df.withColumn(\n",
    "    \"EDUCATION_LEVELS_NAME\",\n",
    "    trim(regexp_replace(col(\"EDUCATION_LEVELS_NAME\"), r\"[\\.\\-]\", \" \"))\n",
    ")\n",
    "\n",
    "regression_df = regression_df.dropna(subset=[\n",
    "    \"SALARY\",\n",
    "    \"MIN_YEARS_EXPERIENCE\",\n",
    "    \"MAX_YEARS_EXPERIENCE\",\n",
    "    \"DURATION\",\n",
    "    \"IS_INTERNSHIP\",\n",
    "    \"COMPANY_IS_STAFFING\",\n",
    "    \"REMOTE_TYPE_NAME\",\n",
    "    \"EMPLOYMENT_TYPE_NAME\",\n",
    "    \"EDUCATION_LEVELS_NAME\"\n",
    "])\n",
    "\n",
    "# Convert Duration to numeric (in days)\n",
    "regression_df =regression_df.withColumn(\"DURATION\", col(\"DURATION\").cast(IntegerType()))\n",
    "\n",
    "regression_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e505c",
   "metadata": {},
   "source": [
    "# Linear Regression Model (OLS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, trim\n",
    "\n",
    "regression_df = regression_df.withColumn(\"EDUCATION_LEVELS_NAME\", trim(regexp_replace(col(\"EDUCATION_LEVELS_NAME\"), r\"[\\.\\-]\", \" \")))\n",
    "\n",
    "# Index and One-Hot Encode\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_idx\", handleInvalid='skip') for col in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{col}_idx\", outputCol=f\"{col}_vec\") for col in categorical_cols]\n",
    "\n",
    "# Assemble base features (for GLR and Random Forrest)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"MIN_YEARS_EXPERIENCE\", \"MAX_YEARS_EXPERIENCE\", \"DURATION\",\n",
    "        \"IS_INTERNSHIP\", \"COMPANY_IS_STAFFING\"\n",
    "    ] + [f\"{col}_vec\" for col in categorical_cols],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "pipeline_reg = Pipeline(stages=indexers + encoders + [assembler])\n",
    "regression_data = pipeline_reg.fit(regression_df).transform(regression_df)\n",
    "\n",
    "regression_data.select(\"SALARY\",\"features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42aa839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "regression_train, regression_test = regression_data.randomSplit([0.8, 0.2], seed=42)\n",
    "print((regression_data.count(), len(regression_data.columns)))\n",
    "print((regression_train.count(), len(regression_train.columns)))\n",
    "print((regression_test.count(), len(regression_test.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ab239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression, LinearRegression\n",
    "\n",
    "feature_names = assembler.getInputCols()\n",
    "\n",
    "glr = GeneralizedLinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"SALARY\",\n",
    "    family=\"gaussian\",\n",
    "    link=\"identity\",\n",
    "    maxIter=10,\n",
    "    regParam=0.3\n",
    ")\n",
    "\n",
    "glr_model = glr.fit(regression_data)\n",
    "summary = glr_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients and Intercept\n",
    "print(\"Intercept: {:.4f}\".format(glr_model.intercept))\n",
    "print(\"Coefficients: \")\n",
    "for i, coef in enumerate(glr_model.coefficients):\n",
    "    print(f\"  Feature {i + 1}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary stats\n",
    "\n",
    "print(\"\\n---Regression Summary---\")\n",
    "print(\"Coefficient Standard Errors:\", [f\"{val:.4f}\" for val in summary.coefficientStandardErrors])\n",
    "print(\"T-values:\", [f\"{val:.4f}\" for val in summary.tValues])\n",
    "print(\"P-values:\", [f\"{val:.4f}\" for val in summary.pValues])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609def41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"nullDeviance: {summary.nullDeviance:.4f}\")\n",
    "print(f\"Residual DF Null: {summary.residualDegreeOfFreedom}\")\n",
    "print(f\"Residual DF: {summary.residualDegreeOfFreedom}\")\n",
    "print(f\"AIC: {summary.aic:.4f}\")\n",
    "print(f\"Deviance: {summary.deviance:.4f}\")\n",
    "explained_variance_deviance = (summary.nullDeviance - summary.deviance) / summary.nullDeviance\n",
    "print(\"Explained Variance (from deviance):\", explained_variance_deviance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34007f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull feature names directly from Java backend\n",
    "\n",
    "feature_names = summary._call_java(\"featureNames\")\n",
    "\n",
    "# Construct full table including intercept\n",
    "features = [\"Intercept\"] + feature_names\n",
    "coef = [glr_model.intercept] + list(glr_model.coefficients)\n",
    "se = list(summary.coefficientStandardErrors)\n",
    "tvals = list(summary.tValues)\n",
    "pvals = list(summary.pValues)\n",
    "\n",
    "print(\"Length of features:\", len(features))\n",
    "print(\"Length of coefficients:\", len(coef))\n",
    "print(\"Length of standard errors:\", len(se))\n",
    "print(\"Length of t-values:\", len(tvals))\n",
    "print(\"Length of p-values:\", len(pvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from IPython.display import HTML\n",
    "\n",
    "coef_table = pd.DataFrame({\n",
    "    \"Features\": features,\n",
    "    \"Estimate\": [f\"{v:.4f}\" if v is not None else None for v in coef],\n",
    "    \"Std. Error\": [f\"{v:.4f}\" if v is not None else None for v in se],\n",
    "    \"t value\": [f\"{v:.4f}\" if v is not None else None for v in tvals],\n",
    "    \"P-value\": [f\"{v:.4f}\" if v is not None else None for v in pvals]\n",
    "})\n",
    "\n",
    "# Save Report\n",
    "coef_table.to_csv(\"output/glr_summary.csb\", index=False)\n",
    "\n",
    "# Optional pretty print\n",
    "\n",
    "HTML (coef_table.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d36f52",
   "metadata": {},
   "source": [
    "# Generalized Linear Regression Summary\n",
    "Based on the features, two significant predictors of salary include remote type name, and employment type name (part time). This makes sense when looking at the data, as remote work tends to pay higher than in person work, so jobs that are broken up by remote, hybrid, or onsite will have different pay scales, while employment type names will be broken up differently because of the difference in salary between part time and full time work. The coefficients have two features that have significant predictors, feature 6 is a positive predictor while feature 10 is a negative predictor. Other features do not have meaningful impacts on the target. The model performance is moderate and could be improved.\n",
    "- Performance\n",
    "    - Based on the Explained Variance this model is calculated to have a ~36% of the varience in salary which indicates that there is a moderate performance, but there is still room for improvement.\n",
    "- Other Features\n",
    "    - Years of experience, Internships, Full time employment are not significant predictors of salary in this model. This does not mean that they do not have significant impacts on salary as a whole, it just means that in terms of this model that there is not a trend that indicates that these features have a significant impact on salary.\n",
    "\n",
    "# Polynomical Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: true\n",
    "#| echo: false\n",
    "#| fig-align: center\n",
    "\n",
    "# Index and One-Hot encode\n",
    "\n",
    "poly_data = regression_data.withColumn(\"MIN_YEARS_EXPERIENCE_SQ\", pow(col(\"MIN_YEARS_EXPERIENCE\"),2))\n",
    "\n",
    "assembler_poly = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"MIN_YEARS_EXPERIENCE\", \"MIN_YEARS_EXPERIENCE_SQ\", \"MAX_YEARS_EXPERIENCE\", \"DURATION\",\n",
    "        \"IS_INTERNSHIP\", \"COMPANY_IS_STAFFING\"\n",
    "    ] + [f\"{col}_vec\" for col in categorical_cols],\n",
    "    outputCol=\"features_poly\"\n",
    ")  \n",
    "\n",
    "poly_data = assembler_poly.transform(poly_data)\n",
    "\n",
    "poly_data.select(\"SALARY\",\"features_poly\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3b5658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "polyreg_train, polyreg_test = poly_data.randomSplit([0.8, 0.2], seed=42)\n",
    "print((poly_data.count(), len(poly_data.columns)))\n",
    "print((polyreg_train.count(), len(polyreg_train.columns)))\n",
    "print((polyreg_test.count(), len(polyreg_test.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8abe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression, LinearRegression\n",
    "\n",
    "feature_names = assembler.getInputCols()\n",
    "\n",
    "poly_glr_max_years_model = GeneralizedLinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"SALARY\",\n",
    "    family=\"gaussian\",\n",
    "    link=\"identity\",\n",
    "    maxIter=10,\n",
    "    regParam=0.3\n",
    ")\n",
    "\n",
    "poly_glr_max_years_model = poly_glr_max_years_model.fit(poly_data)\n",
    "poly_summary = poly_glr_max_years_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients and Intercept\n",
    "print(\"Intercept: {:.4f}\".format(poly_glr_max_years_model.intercept))\n",
    "print(\"Coefficients: \")\n",
    "for i, coef in enumerate(poly_glr_max_years_model.coefficients):\n",
    "    print(f\"  Feature {i + 1}: {coef:.4f}\")\n",
    "\n",
    "    # Summary stats\n",
    "\n",
    "print(\"\\n---Regression Summary---\")\n",
    "print(\"Coefficient Standard Errors:\", [f\"{val:.4f}\" for val in poly_summary.coefficientStandardErrors])\n",
    "print(\"T-values:\", [f\"{val:.4f}\" for val in poly_summary.tValues])\n",
    "print(\"P-values:\", [f\"{val:.4f}\" for val in poly_summary.pValues])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c47830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"nullDeviance: {poly_summary.nullDeviance:.4f}\")\n",
    "print(f\"Residual DF Null: {poly_summary.residualDegreeOfFreedom}\")\n",
    "print(f\"Residual DF: {poly_summary.residualDegreeOfFreedom}\")\n",
    "print(f\"AIC: {poly_summary.aic:.4f}\")\n",
    "print(f\"Deviance: {poly_summary.deviance:.4f}\")\n",
    "poly_explained_variance_deviance = (poly_summary.nullDeviance - poly_summary.deviance) / poly_summary.nullDeviance\n",
    "print(\"Explained Variance (from deviance):\", poly_explained_variance_deviance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c135d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull feature names directly from Java backend\n",
    "\n",
    "feature_names = poly_summary._call_java(\"featureNames\")\n",
    "\n",
    "# Construct full table including intercept\n",
    "poly_features = [\"Intercept\"] + feature_names\n",
    "poly_coef = [poly_glr_max_years_model.intercept] + list(poly_glr_max_years_model.coefficients)\n",
    "poly_se = list(poly_summary.coefficientStandardErrors)\n",
    "poly_tvals = list(poly_summary.tValues)\n",
    "poly_pvals = list(poly_summary.pValues)\n",
    "\n",
    "print(\"Length of features:\", len(poly_features))\n",
    "print(\"Length of coefficients:\", len(poly_coef))\n",
    "print(\"Length of standard errors:\", len(poly_se))\n",
    "print(\"Length of t-values:\", len(poly_tvals))\n",
    "print(\"Length of p-values:\", len(poly_pvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4fd10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from IPython.display import HTML\n",
    "\n",
    "poly_coef_table = pd.DataFrame({\n",
    "    \"Features\": poly_features,\n",
    "    \"Estimate\": [f\"{v:.4f}\" if v is not None else None for v in poly_coef],\n",
    "    \"Std. Error\": [f\"{v:.4f}\" if v is not None else None for v in poly_se],\n",
    "    \"t value\": [f\"{v:.4f}\" if v is not None else None for v in poly_tvals],\n",
    "    \"P-value\": [f\"{v:.4f}\" if v is not None else None for v in poly_pvals]\n",
    "})\n",
    "\n",
    "# Save Report\n",
    "poly_coef_table.to_csv(\"output/poly_summary.csb\", index=False)\n",
    "\n",
    "# Optional pretty print\n",
    "\n",
    "HTML (poly_coef_table.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf659c9c",
   "metadata": {},
   "source": [
    "# Poly Regression Summary\n",
    "\n",
    "This data has similar impacts to the Linear regression model, Remote_TYPE_NAME having a positive impact and Employment_TYPE_NAME (part time) have a negative impact which are significant predictors of salary according to this model. While other features are not significant predictors. The model performance is the same based on the explained varience which is at 36%.\n",
    "\n",
    "# Random Forest Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create the Random Forest Regressor model\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"SALARY\",   \n",
    "    numTrees=300,        \n",
    "    maxDepth=6,          \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train the model on training data\n",
    "rf_model = rf.fit(polyreg_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "rf_predictions = rf_model.transform(polyreg_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"SALARY\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(rf_predictions)\n",
    "r2 = evaluator.evaluate(rf_predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(f\"Random Forest RMSE: {rmse:.4f}\")\n",
    "print(f\"Random Forest RÂ²: {r2:.4f}\")\n",
    "\n",
    "\n",
    "print(\"Feature Importances:\", rf_model.featureImportances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d50b6",
   "metadata": {},
   "source": [
    "# Feature Importance Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f368680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "importances = rf_model.featureImportances.toArray()\n",
    "\n",
    "min_len = min(len(feature_names), len(importances))\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names[:min_len],\n",
    "    \"Importance\": importances[:min_len]\n",
    "})\n",
    "\n",
    "top10 = importance_df.sort_values(by=\"Importance\", ascending=False).head(10)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=top10, palette=\"viridis\")\n",
    "plt.title(\"Top 10 Most Important Features - Random Forest\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Used AI to help me display dataframe as I was getting the error for length mismatch (See Appendix 1 in AI_Prompts.qmd)\n",
    "# Matching lengths to make sure dataframe correctly renders\n",
    "min_len = min(len(feature_names), len(importances))\n",
    "feature_names, importances = feature_names[:min_len], importances[:min_len]\n",
    "\n",
    "featureimportanceplot_df= pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "featureimportanceplot_df = featureimportanceplot_df.head(10)\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(\"_output/rf_feature_importance.png\", dpi=300)\n",
    "plt.show()\n",
    "featureimportanceplot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db7c9c",
   "metadata": {},
   "source": [
    "# Compare 3 Models - GLR,  Polynomial, RF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf75d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predictions for each model\n",
    "glr_predictions = glr_model.transform(regression_data)\n",
    "poly_predictions = poly_glr_max_years_model.transform(poly_data)\n",
    "rf_predictions = rf_model.transform(polyreg_test)\n",
    "\n",
    "# RMSE for each model\n",
    "evaluator = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "glr_rmse = evaluator.evaluate(glr_predictions)\n",
    "poly_rmse = evaluator.evaluate(poly_predictions)\n",
    "rf_rmse = evaluator.evaluate(rf_predictions)\n",
    "\n",
    "# AIC and BIC for each Model\n",
    "# AIC is already available in the summary for GLR models\n",
    "glr_aic = summary.aic\n",
    "poly_aic = poly_summary.aic\n",
    "\n",
    "# Manual BIC calculation\n",
    "def calculate_bic(n, mse, k):\n",
    "    return n * np.log(mse) + k * np.log(n)\n",
    "\n",
    "# Count observations and parameters\n",
    "n_glr = glr_predictions.count()\n",
    "n_poly = poly_predictions.count()\n",
    "n_rf = rf_predictions.count()\n",
    "\n",
    "# Compute mean squared error (for BIC)\n",
    "glr_mse = glr_rmse**2\n",
    "poly_mse = poly_rmse**2\n",
    "rf_mse = rf_rmse**2\n",
    "\n",
    "# Number of parameters (approx: # features + intercept)\n",
    "k_glr = len(glr_model.coefficients) + 1\n",
    "k_poly = len(poly_glr_max_years_model.coefficients) + 1\n",
    "k_rf = len(rf_model.featureImportances)\n",
    "\n",
    "# Compute BIC\n",
    "glr_bic = calculate_bic(n_glr, glr_mse, k_glr)\n",
    "poly_bic = calculate_bic(n_poly, poly_mse, k_poly)\n",
    "rf_bic = calculate_bic(n_rf, rf_mse, k_rf)\n",
    "\n",
    "# Summarized model metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Model\": [\"GLR\", \"Polynomial GLR\", \"Random Forest\"],\n",
    "    \"RMSE\": [glr_rmse, poly_rmse, rf_rmse],\n",
    "    \"AIC\": [glr_aic, poly_aic, np.nan],  # AIC not standard for RF\n",
    "    \"BIC\": [glr_bic, poly_bic, rf_bic]\n",
    "})\n",
    "\n",
    "print(\"Model Comparison Metrics:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Pandas DataFrame for visualization\n",
    "glr_pd = glr_predictions.select(\"SALARY\", \"prediction\").toPandas().assign(Model=\"GLR\")\n",
    "poly_pd = poly_predictions.select(\"SALARY\", \"prediction\").toPandas().assign(Model=\"Polynomial\")\n",
    "rf_pd = rf_predictions.select(\"SALARY\", \"prediction\").toPandas().assign(Model=\"Random Forest\")\n",
    "\n",
    "combined_pd = pd.concat([glr_pd, poly_pd, rf_pd])\n",
    "\n",
    "# Plot Actual vs Predicted in a 2x2 grid ---\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "models = [\"GLR\", \"Polynomial\", \"Random Forest\"]\n",
    "for i, model in enumerate(models):\n",
    "    ax = axes.flat[i]\n",
    "    subset = combined_pd[combined_pd[\"Model\"] == model]\n",
    "    sns.scatterplot(x=\"SALARY\", y=\"prediction\", data=subset, alpha=0.6, ax=ax)\n",
    "    sns.lineplot(x=subset[\"SALARY\"], y=subset[\"SALARY\"], color=\"red\", ax=ax, label=\"Ideal Fit\")\n",
    "    ax.set_title(f\"{model}: Actual vs Predicted\")\n",
    "    ax.set_xlabel(\"Actual Salary\")\n",
    "    ax.set_ylabel(\"Predicted Salary\")\n",
    "\n",
    "# Hide the 4th empty subplot\n",
    "axes.flat[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c0b49",
   "metadata": {},
   "source": [
    "# Calcutlating Log-Likelihood and BIC for Pyspark Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b58ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_likelihood(n, deviance, dispersion):\n",
    "    \"\"\"\n",
    "    Compute log-likelihood for GLR (Gaussian / identity link)\n",
    "    Formula: -0.5 * (n*log(2*pi) + n*log(dispersion) + deviance/dispersion)\n",
    "    \"\"\"\n",
    "    return -0.5 * (n * np.log(2 * np.pi) + n * np.log(dispersion) + deviance / dispersion)\n",
    "\n",
    "def bic(n, k, ll):\n",
    "    \"\"\"\n",
    "    Compute BIC\n",
    "    BIC = k * log(n) - 2 * log-likelihood\n",
    "    \"\"\"\n",
    "    return k * np.log(n) - 2 * ll\n",
    "\n",
    "# --- GLR Model ---\n",
    "n_glr = glr_predictions.count()\n",
    "deviance_glr = summary.deviance\n",
    "dispersion_glr = deviance_glr / summary.residualDegreeOfFreedom\n",
    "ll_glr = log_likelihood(n_glr, deviance_glr, dispersion_glr)\n",
    "k_glr = len(glr_model.coefficients) + 1  # coefficients + intercept\n",
    "bic_glr = bic(n_glr, k_glr, ll_glr)\n",
    "\n",
    "# --- Polynomial GLR Model ---\n",
    "n_poly = poly_predictions.count()\n",
    "deviance_poly = poly_summary.deviance\n",
    "dispersion_poly = deviance_poly / poly_summary.residualDegreeOfFreedom\n",
    "ll_poly = log_likelihood(n_poly, deviance_poly, dispersion_poly)\n",
    "k_poly = len(poly_glr_max_years_model.coefficients) + 1\n",
    "bic_poly = bic(n_poly, k_poly, ll_poly)\n",
    "\n",
    "print(f\"GLR BIC: {bic_glr:.4f}\")\n",
    "print(f\"Polynomial GLR BIC: {bic_poly:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
